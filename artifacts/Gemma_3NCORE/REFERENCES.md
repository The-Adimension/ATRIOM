**Disclaimer**: For research only; not for clinical use. Follow providers' ethical terms for models/datasets [1-12].
---
# **References**
1. Gemma Team. (2025). Gemma 3n. Google DeepMind. https://ai.google.dev/gemma/docs/gemma-3n.
2. Anwer, S. (2025). The Adimension: Bridging interoperability through DEITY Principles. *European Heart Journal - Imaging Methods and Practice*. https://doi.org/10.1093/ehjimp/qyaf038.
3. Sellergren, A., et al. (2025). MedGemma Technical Report. arXiv:2507.05201.
4. Wightman, R. (2019). PyTorch Image Models. https://github.com/huggingface/pytorch-image-models.
5. Devvrit et al. (2023). MatFormer: Nested Transformer for Elastic Inference. arXiv:2310.07707.
6. Ouyang, D., et al. (2020). Video-based AI for beat-to-beat assessment of cardiac function. *Nature*. https://doi.org/10.1038/s41586-020-2145-8 (EchoNet-Dynamic Dataset).
7. Leclerc, S., et al. (2019). Deep Learning for Segmentation using an Open Large-Scale Dataset in 2D Echocardiography. *IEEE Transactions on Medical Imaging*. https://doi.org/10.1109/TMI.2019.2900516 (CAMUS Dataset).
8. Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685.
9. Dettmers, T., et al. (2022). LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv:2208.07339.
10. Paszke, A., et al. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. *NeurIPS*.
11. Wolf, T., et al. (2019). Hugging Face's Transformers. arXiv:1910.03771.
12. Google Health AI Team. (2024). Health AI Developer Foundations. arXiv:2411.15128.